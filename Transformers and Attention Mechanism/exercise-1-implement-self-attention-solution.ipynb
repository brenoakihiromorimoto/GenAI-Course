{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f54f39-d32a-4173-a35f-cfd6581d7617",
   "metadata": {},
   "source": [
    "# Exercise: Implement Self-Attention (Solution)\n",
    "\n",
    "In this exercise you will be implementing the `forward()` function of the `MultiHeadSelfAttention` module in a minified GPT implementation. GPT refers to the \"Generative Pre-trained Transformers\" from OpenAI, originally described in [\"Improving language understanding with unsupervised learning\"](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf). This specific GPT implementation is heavily inspired by the [minGPT implementation](https://github.com/karpathy/minGPT) provided by [Andrej Karpathy](https://github.com/karpathy/).\n",
    "\n",
    "\n",
    "Let's dive into the `MultiHeadSelfAttention` module, which is central to the overall GPT model.\n",
    "\n",
    "## Scaled Multiplicative Attention\n",
    "\n",
    "Recall this attention formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "This is represented in the Python code as follows:\n",
    "\n",
    "* $Q$: `q`\n",
    "* $K$: `k`\n",
    "* $V$: `v`\n",
    "* $\\text{softmax}$: `F.softmax()`\n",
    "* $K^T$: `k_t`\n",
    "* $QK^T$ (matrix multiplication): `q @ k_t`\n",
    "* $\\sqrt{}$: `math.sqrt()`\n",
    "* $d_k$: `d_k`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2f90f",
   "metadata": {},
   "source": [
    "## Your Task: Complete the MultiHeadSelfAttention class\n",
    "\n",
    "Within the `MultiHeadSelfAttention` class, fill in the parts marked `TODO` within the between the lines marked `EXERCISE START` and `EXERCISE END`.\n",
    "\n",
    "Please also take the opportunity to read through the class to understand its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed032feb-890b-4cf9-97ae-100938a360fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"A vanilla multi-head masked self-attention layer.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward pass for the multi-head masked self-attention layer.\n",
    "\n",
    "        In this exercise, we include lots of print statements and checks to help you\n",
    "        understand the code and the shapes of the tensors. When actually training\n",
    "        such a model you would not log this information to the console.\n",
    "        \"\"\"\n",
    "\n",
    "        # batch size, sequence length (in tokens), embedding dimensionality (n_embd per token)\n",
    "        B, T, C = x.size()\n",
    "        hs = C // self.n_head  # head size\n",
    "\n",
    "        # print some debug information\n",
    "        print(f\"batch size: {B}\")\n",
    "        print(f\"sequence length: {T}\")\n",
    "        print(f\"embedding dimensionality: {C}\")\n",
    "        print(f\"number of heads: {self.n_head}\")\n",
    "        print(f\"head size: {hs}\")\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # resulting dims for k, q, and v are (B, n_head, T, hs)\n",
    "        k = self.key(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "\n",
    "        # === EXERCISE START: IMPLEMENT THE MULTI-HEAD ATTENTION ===\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO: multiply q and k_t matrices, then divide by the square root of d_k\n",
    "        print(\"=== Calculate MatrixMultiplication(Q, K_T) / sqrt(d_k) ===\")\n",
    "\n",
    "        k_t = k.transpose(-2, -1)  # what is the shape of k_t?\n",
    "        d_k = k.size(-1)\n",
    "\n",
    "        # Matrix multiplication (hint: not \"*\")\n",
    "        # att = <TODO>\n",
    "        att = q @ k_t / math.sqrt(d_k)  # Solution\n",
    "\n",
    "        print(f\"q.shape: {q.shape}\")\n",
    "        print(f\"k_t.shape: {k_t.shape}\")\n",
    "        print(f\"d_k: {d_k}\")\n",
    "        print(f\"att.shape: {att.shape}\")\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO: set the mask fill value to negative infinity\n",
    "        print(\"=== Apply the attention mask ===\")\n",
    "\n",
    "        # masked_fill_value = <TODO>\n",
    "        masked_fill_value = float(\"-inf\")  # Solution\n",
    "\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, masked_fill_value)\n",
    "\n",
    "        # Show the result of applying the mask\n",
    "        print(f\"att: {att}\")\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO: apply softmax\n",
    "        print(\"=== Softmax ===\")\n",
    "\n",
    "        # att = F.softmax(att, dim=<TODO>)\n",
    "        att = F.softmax(att, dim=-1)  # Solution\n",
    "\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        # Show the result of applying the softmax and check that\n",
    "        # the sum of the attention weights in each row is 1\n",
    "        print(f\"att.shape: {att.shape}\")\n",
    "        print(f\"att: {att}\")\n",
    "        print(f\"att.sum(dim=-1): {att.sum(dim=-1)}\")\n",
    "        att_rows_sum_to_one = all(\n",
    "            ((att.sum(dim=-1) - 1.0) ** 2 < 1e-6).flatten().tolist()\n",
    "        )\n",
    "        print(f\"att_rows_sum_to_one: {att_rows_sum_to_one}\")\n",
    "        if not att_rows_sum_to_one:\n",
    "            raise ValueError(\n",
    "                \"Attention weight rows do not sum to 1. Perhaps the softmax dimension or masked_fill_value is not correct?\"\n",
    "            )\n",
    "\n",
    "        ######################################################################\n",
    "        # TODO: multiply att and v matrices\n",
    "        # (B, n_head, T, T) x (B, n_head, T, hs) -> (B, n_head, T, hs)\n",
    "        print(\"=== Calculate final attention ===\")\n",
    "\n",
    "        # y = <TODO>\n",
    "        y = att @ v  # Solution\n",
    "\n",
    "        print(f\"y.shape: {y.shape}\")\n",
    "\n",
    "        ######################################################################\n",
    "\n",
    "        # === EXERCISE END: IMPLEMENT THE MULTI-HEAD ATTENTION ===\n",
    "\n",
    "        # re-assemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c86ae",
   "metadata": {},
   "source": [
    "**Let's check the implementation.** No modifications are needed for the next cell. Please run it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe0376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "sequence length: 5\n",
      "embedding dimensionality: 12\n",
      "number of heads: 4\n",
      "head size: 3\n",
      "=== Calculate MatrixMultiplication(Q, K_T) / sqrt(d_k) ===\n",
      "q.shape: torch.Size([1, 4, 5, 3])\n",
      "k_t.shape: torch.Size([1, 4, 3, 5])\n",
      "d_k: 3\n",
      "att.shape: torch.Size([1, 4, 5, 5])\n",
      "=== Apply the attention mask ===\n",
      "att: tensor([[[[16.6450,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,    -inf,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,  2.9272,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,  2.9272,  2.9272]],\n",
      "\n",
      "         [[16.6450,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,    -inf,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,  2.9272,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,  2.9272,  2.9272]],\n",
      "\n",
      "         [[16.6450,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,    -inf,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,  2.9272,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,  2.9272,  2.9272]],\n",
      "\n",
      "         [[16.6450,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,    -inf,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,    -inf,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,  2.9272,    -inf],\n",
      "          [ 6.9802,  2.9272,  2.9272,  2.9272,  2.9272]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "=== Softmax ===\n",
      "att.shape: torch.Size([1, 4, 5, 5])\n",
      "att: tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9829, 0.0171, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9664, 0.0168, 0.0168, 0.0000, 0.0000],\n",
      "          [0.9505, 0.0165, 0.0165, 0.0165, 0.0000],\n",
      "          [0.9350, 0.0162, 0.0162, 0.0162, 0.0162]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9829, 0.0171, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9664, 0.0168, 0.0168, 0.0000, 0.0000],\n",
      "          [0.9505, 0.0165, 0.0165, 0.0165, 0.0000],\n",
      "          [0.9350, 0.0162, 0.0162, 0.0162, 0.0162]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9829, 0.0171, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9664, 0.0168, 0.0168, 0.0000, 0.0000],\n",
      "          [0.9505, 0.0165, 0.0165, 0.0165, 0.0000],\n",
      "          [0.9350, 0.0162, 0.0162, 0.0162, 0.0162]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9829, 0.0171, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9664, 0.0168, 0.0168, 0.0000, 0.0000],\n",
      "          [0.9505, 0.0165, 0.0165, 0.0165, 0.0000],\n",
      "          [0.9350, 0.0162, 0.0162, 0.0162, 0.0162]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "att.sum(dim=-1): tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]], grad_fn=<SumBackward1>)\n",
      "att_rows_sum_to_one: True\n",
      "=== Calculate final attention ===\n",
      "y.shape: torch.Size([1, 4, 5, 3])\n",
      "=== Showing the input and output ===\n",
      "tensor([[[1., 1., 1., 2., 2., 2., 3., 3., 3., 4., 4., 4.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "tensor([[[3.8200, 3.8200, 3.8200, 3.8200, 3.8200, 3.8200, 3.8200, 3.8200,\n",
      "          3.8200, 3.8200, 3.8200, 3.8200],\n",
      "         [3.7831, 3.7831, 3.7831, 3.7831, 3.7831, 3.7831, 3.7831, 3.7831,\n",
      "          3.7831, 3.7831, 3.7831, 3.7831],\n",
      "         [3.7475, 3.7475, 3.7475, 3.7475, 3.7475, 3.7475, 3.7475, 3.7475,\n",
      "          3.7475, 3.7475, 3.7475, 3.7475],\n",
      "         [3.7130, 3.7130, 3.7130, 3.7130, 3.7130, 3.7130, 3.7130, 3.7130,\n",
      "          3.7130, 3.7130, 3.7130, 3.7130],\n",
      "         [3.6797, 3.6797, 3.6797, 3.6797, 3.6797, 3.6797, 3.6797, 3.6797,\n",
      "          3.6797, 3.6797, 3.6797, 3.6797]]], grad_fn=<ViewBackward0>)\n",
      "=== Checking gradients ===\n",
      "Gradients: [161, 13, 294, 0, 37187, 432]\n",
      "Success! 🚀🚀🚀\n"
     ]
    }
   ],
   "source": [
    "class GPTConfig:\n",
    "    vocab_size = 11\n",
    "    block_size = 5\n",
    "    # n_layer = 1  # not used in this exercise\n",
    "    n_head = 4\n",
    "    n_embd = 12\n",
    "\n",
    "    attn_pdrop = 0.0\n",
    "    resid_pdrop = 0.0\n",
    "\n",
    "\n",
    "# set pytorch seed for reproducibility\n",
    "attention = MultiHeadSelfAttention(GPTConfig())\n",
    "x = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            # 12-dimensional embeddings (4 heads @ 3 dim. each) for each of 5 tokens\n",
    "            [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0],\n",
    "            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set all parameters to 0.1\n",
    "for weight in attention.parameters():\n",
    "    nn.init.constant_(weight, 0.1)\n",
    "\n",
    "# Set the model to evaluation mode to disable dropout\n",
    "# attention.eval()\n",
    "\n",
    "# Perform a forward pass\n",
    "y = attention(x)\n",
    "\n",
    "# Check that the output shape is correct\n",
    "assert y.shape == x.shape\n",
    "\n",
    "\n",
    "print(\"=== Showing the input and output ===\")\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "\n",
    "print(\"=== Checking gradients ===\")\n",
    "\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "# check if nan in y\n",
    "if torch.isnan(y).any().item():\n",
    "    raise ValueError(\n",
    "        \"It appears that the output contains NaNs. Perhaps the softmax dimension is incorrect?\"\n",
    "    )\n",
    "\n",
    "gradients = [\n",
    "    int((attention.query.weight.grad**2).sum().item()),\n",
    "    int((attention.query.bias.grad**2).sum().item()),\n",
    "    int((attention.key.weight.grad**2).sum().item()),\n",
    "    int((attention.key.bias.grad**2).sum().item()),\n",
    "    int((attention.value.weight.grad**2).sum().item()),\n",
    "    int((attention.value.bias.grad**2).sum().item()),\n",
    "]\n",
    "\n",
    "print(\"Gradients:\", gradients)\n",
    "if gradients == [161, 13, 294, 0, 37187, 432]:\n",
    "    print(\"Success! 🚀🚀🚀\")\n",
    "elif gradients == [1, 0, 2, 0, 38787, 432]:\n",
    "    raise RuntimeError(\n",
    "        \"There is an error in your implementation. Please check your code. Did you remember to divide by the square root of d_k?\"\n",
    "    )\n",
    "elif gradients[-1] == 432:\n",
    "    raise RuntimeError(\n",
    "        \"There is an error in your implementation. Please check your code. Did you use -inf as the masked_fill_value?\"\n",
    "    )\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"There is an error in your implementation. Please check your code.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545351ca",
   "metadata": {},
   "source": [
    "## Congrats!\n",
    "\n",
    "You have implemented your own self-attention module for a GPT model. Great work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
